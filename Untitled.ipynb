{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5fa4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"sentiment.py - analyze tweets on Twitter and add\n",
    "relevant tweets and their sentiment values to\n",
    "Elasticsearch.\n",
    "See README.md or https://github.com/shirosaidev/stocksight\n",
    "for more information.\n",
    "\n",
    "Copyright (C) Chris Park 2018-2020\n",
    "stocksight is released under the Apache 2.0 license. See\n",
    "LICENSE for the full license text.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import nltk\n",
    "import argparse\n",
    "import logging\n",
    "import string\n",
    "try:\n",
    "    import urllib.parse as urlparse\n",
    "except ImportError:\n",
    "    import urlparse\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import API, Stream, OAuthHandler, TweepError\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from bs4 import BeautifulSoup\n",
    "from elasticsearch import Elasticsearch\n",
    "from random import randint, randrange\n",
    "from datetime import datetime\n",
    "from newspaper import Article, ArticleException\n",
    "\n",
    "# import elasticsearch host, twitter keys and tokens\n",
    "from config import *\n",
    "\n",
    "\n",
    "STOCKSIGHT_VERSION = '0.1-b.12'\n",
    "__version__ = STOCKSIGHT_VERSION\n",
    "\n",
    "IS_PY3 = sys.version_info >= (3, 0)\n",
    "\n",
    "if not IS_PY3:\n",
    "    print(\"Sorry, stocksight requires Python 3.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# sentiment text-processing url\n",
    "sentimentURL = 'http://text-processing.com/api/sentiment/'\n",
    "\n",
    "# tweet id list\n",
    "tweet_ids = []\n",
    "\n",
    "# file to hold twitter user ids\n",
    "twitter_users_file = './twitteruserids.txt'\n",
    "\n",
    "prev_time = time.time()\n",
    "sentiment_avg = [0.0,0.0,0.0]\n",
    "\n",
    "\n",
    "class TweetStreamListener(StreamListener):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.count_filtered = 0\n",
    "        self.filter_ratio = 0\n",
    "\n",
    "    # on success\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            self.count+=1\n",
    "            # decode json\n",
    "            dict_data = json.loads(data)\n",
    "\n",
    "            print(\"\\n------------------------------> (tweets: %s, filtered: %s, filter-ratio: %s)\" \\\n",
    "                % (self.count, self.count_filtered, str(round(self.count_filtered/self.count*100,2))+\"%\"))\n",
    "            logger.debug('tweet data: ' + str(dict_data))\n",
    "\n",
    "            text = dict_data[\"text\"]\n",
    "            if text is None:\n",
    "                logger.info(\"Tweet has no relevant text, skipping\")\n",
    "                self.count_filtered+=1\n",
    "                return True\n",
    "\n",
    "            # grab html links from tweet\n",
    "            tweet_urls = []\n",
    "            if args.linksentiment:\n",
    "                tweet_urls = re.findall(r'(https?://[^\\s]+)', text)\n",
    "\n",
    "            # clean up tweet text\n",
    "            textclean = clean_text(text)\n",
    "\n",
    "            # check if tweet has no valid text\n",
    "            if textclean == \"\":\n",
    "                logger.info(\"Tweet does not cotain any valid text after cleaning, not adding\")\n",
    "                self.count_filtered+=1\n",
    "                return True\n",
    "\n",
    "            # get date when tweet was created\n",
    "            created_date = time.strftime(\n",
    "                '%Y-%m-%dT%H:%M:%S', time.strptime(dict_data['created_at'], '%a %b %d %H:%M:%S +0000 %Y'))\n",
    "\n",
    "            # store dict_data into vars\n",
    "            screen_name = str(dict_data.get(\"user\", {}).get(\"screen_name\"))\n",
    "            location = str(dict_data.get(\"user\", {}).get(\"location\"))\n",
    "            language = str(dict_data.get(\"user\", {}).get(\"lang\"))\n",
    "            friends = int(dict_data.get(\"user\", {}).get(\"friends_count\"))\n",
    "            followers = int(dict_data.get(\"user\", {}).get(\"followers_count\"))\n",
    "            statuses = int(dict_data.get(\"user\", {}).get(\"statuses_count\"))\n",
    "            text_filtered = str(textclean)\n",
    "            tweetid = int(dict_data.get(\"id\"))\n",
    "            text_raw = str(dict_data.get(\"text\"))\n",
    "\n",
    "            # output twitter data\n",
    "            print(\"\\n<------------------------------\")\n",
    "            print(\"Tweet Date: \" + created_date)\n",
    "            print(\"Screen Name: \" + screen_name)\n",
    "            print(\"Location: \" + location)\n",
    "            print(\"Language: \" + language)\n",
    "            print(\"Friends: \" + str(friends))\n",
    "            print(\"Followers: \" + str(followers))\n",
    "            print(\"Statuses: \" + str(statuses))\n",
    "            print(\"Tweet ID: \" + str(tweetid))\n",
    "            print(\"Tweet Raw Text: \" + text_raw)\n",
    "            print(\"Tweet Filtered Text: \" + text_filtered)\n",
    "\n",
    "            # create tokens of words in text using nltk\n",
    "            text_for_tokens = re.sub(\n",
    "                r\"[\\%|\\$|\\.|\\,|\\!|\\:|\\@]|\\(|\\)|\\#|\\+|(``)|('')|\\?|\\-\", \"\", text_filtered)\n",
    "            tokens = nltk.word_tokenize(text_for_tokens)\n",
    "            # convert to lower case\n",
    "            tokens = [w.lower() for w in tokens]\n",
    "            # remove punctuation from each word\n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            stripped = [w.translate(table) for w in tokens]\n",
    "            # remove remaining tokens that are not alphabetic\n",
    "            tokens = [w for w in stripped if w.isalpha()]\n",
    "            # filter out stop words\n",
    "            stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "            tokens = [w for w in tokens if not w in stop_words]\n",
    "            # remove words less than 3 characters\n",
    "            tokens = [w for w in tokens if not len(w) < 3]\n",
    "            print(\"NLTK Tokens: \" + str(tokens))\n",
    "\n",
    "            # check for min token length\n",
    "            if len(tokens) < 5:\n",
    "                logger.info(\"Tweet does not contain min. number of tokens, not adding\")\n",
    "                self.count_filtered+=1\n",
    "                return True\n",
    "\n",
    "            # do some checks before adding to elasticsearch and crawling urls in tweet\n",
    "            if friends == 0 or \\\n",
    "                            followers == 0 or \\\n",
    "                            statuses == 0 or \\\n",
    "                            text == \"\" or \\\n",
    "                            tweetid in tweet_ids:\n",
    "                logger.info(\"Tweet doesn't meet min requirements, not adding\")\n",
    "                self.count_filtered+=1\n",
    "                return True\n",
    "\n",
    "            # check ignored tokens from config\n",
    "            for t in nltk_tokens_ignored:\n",
    "                if t in tokens:\n",
    "                    logger.info(\"Tweet contains token from ignore list, not adding\")\n",
    "                    self.count_filtered+=1\n",
    "                    return True\n",
    "            # check required tokens from config\n",
    "            tokenspass = False\n",
    "            tokensfound = 0\n",
    "            for t in nltk_tokens_required:\n",
    "                if t in tokens:\n",
    "                    tokensfound += 1\n",
    "                    if tokensfound == nltk_min_tokens:\n",
    "                        tokenspass = True\n",
    "                        break\n",
    "            if not tokenspass:\n",
    "                logger.info(\"Tweet does not contain token from required list or min required, not adding\")\n",
    "                self.count_filtered+=1\n",
    "                return True\n",
    "\n",
    "            # clean text for sentiment analysis\n",
    "            text_clean = clean_text_sentiment(text_filtered)\n",
    "\n",
    "            # check if tweet has no valid text\n",
    "            if text_clean == \"\":\n",
    "                logger.info(\"Tweet does not cotain any valid text after cleaning, not adding\")\n",
    "                self.count_filtered+=1\n",
    "                return True\n",
    "\n",
    "            print(\"Tweet Clean Text (sentiment): \" + text_clean)\n",
    "\n",
    "            # get sentiment values\n",
    "            polarity, subjectivity, sentiment = sentiment_analysis(text_clean)\n",
    "\n",
    "            # add tweet_id to list\n",
    "            tweet_ids.append(dict_data[\"id\"])\n",
    "\n",
    "            # get sentiment for tweet \n",
    "            if len(tweet_urls) > 0:\n",
    "                tweet_urls_polarity = 0\n",
    "                tweet_urls_subjectivity = 0\n",
    "                for url in tweet_urls:\n",
    "                    res = tweeklink_sentiment_analysis(url)\n",
    "                    if res is None:\n",
    "                        continue\n",
    "                    pol, sub, sen = res\n",
    "                    tweet_urls_polarity = (tweet_urls_polarity + pol) / 2\n",
    "                    tweet_urls_subjectivity = (tweet_urls_subjectivity + sub) / 2\n",
    "                    if sentiment == \"positive\" or sen == \"positive\":\n",
    "                        sentiment = \"positive\"\n",
    "                    elif sentiment == \"negative\" or sen == \"negative\":\n",
    "                        sentiment = \"negative\"\n",
    "                    else:\n",
    "                        sentiment = \"neutral\"\n",
    "\n",
    "                # calculate average polarity and subjectivity from tweet and tweet links\n",
    "                if tweet_urls_polarity > 0:\n",
    "                    polarity = (polarity + tweet_urls_polarity) / 2\n",
    "                if tweet_urls_subjectivity > 0:\n",
    "                    subjectivity = (subjectivity + tweet_urls_subjectivity) / 2\n",
    "            \n",
    "\n",
    "            logger.info(\"Adding tweet to elasticsearch\")\n",
    "            # add twitter data and sentiment info to elasticsearch\n",
    "            es.index(index=args.index,\n",
    "                    doc_type=\"tweet\",\n",
    "                    body={\"author\": screen_name,\n",
    "                        \"location\": location,\n",
    "                        \"language\": language,\n",
    "                        \"friends\": friends,\n",
    "                        \"followers\": followers,\n",
    "                        \"statuses\": statuses,\n",
    "                        \"date\": created_date,\n",
    "                        \"message\": text_filtered,\n",
    "                        \"tweet_id\": tweetid,\n",
    "                        \"polarity\": polarity,\n",
    "                        \"subjectivity\": subjectivity,\n",
    "                        \"sentiment\": sentiment})\n",
    "\n",
    "            # randomly sleep to stagger request time\n",
    "            time.sleep(randrange(2,5))\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Exception: exception caused by: %s\" % e)\n",
    "            raise\n",
    "\n",
    "    # on failure\n",
    "    def on_error(self, status_code):\n",
    "        logger.error(\"Got an error with status code: %s (will try again later)\" % status_code)\n",
    "        # randomly sleep to stagger request time\n",
    "        time.sleep(randrange(2,30))\n",
    "        return True\n",
    "\n",
    "    # on timeout\n",
    "    def on_timeout(self):\n",
    "        logger.warning(\"Timeout... (will try again later)\")\n",
    "        # randomly sleep to stagger request time\n",
    "        time.sleep(randrange(2,30))\n",
    "        return True\n",
    "\n",
    "\n",
    "class NewsHeadlineListener:\n",
    "\n",
    "    def __init__(self, url=None, frequency=120):\n",
    "        self.url = url\n",
    "        self.headlines = []\n",
    "        self.followedlinks = []\n",
    "        self.frequency = frequency\n",
    "        self.count = 0\n",
    "        self.count_filtered = 0\n",
    "        self.filter_ratio = 0\n",
    "\n",
    "        while True:\n",
    "            new_headlines = self.get_news_headlines(self.url)\n",
    "\n",
    "            # add any new headlines\n",
    "            for htext, htext_url in new_headlines:\n",
    "                if htext not in self.headlines:\n",
    "                    self.headlines.append(htext)\n",
    "                    self.count+=1\n",
    "\n",
    "                    datenow = datetime.utcnow().isoformat()\n",
    "                    # output news data\n",
    "                    print(\"\\n------------------------------> (news headlines: %s, filtered: %s, filter-ratio: %s)\" \\\n",
    "                        % (self.count, self.count_filtered, str(round(self.count_filtered/self.count*100,2))+\"%\"))\n",
    "                    print(\"Date: \" + datenow)\n",
    "                    print(\"News Headline: \" + htext)\n",
    "                    print(\"Location (url): \" + htext_url)\n",
    "\n",
    "                    # create tokens of words in text using nltk\n",
    "                    text_for_tokens = re.sub(\n",
    "                        r\"[\\%|\\$|\\.|\\,|\\!|\\:|\\@]|\\(|\\)|\\#|\\+|(``)|('')|\\?|\\-\", \"\", htext)\n",
    "                    tokens = nltk.word_tokenize(text_for_tokens)\n",
    "                    print(\"NLTK Tokens: \" + str(tokens))\n",
    "\n",
    "                    # check for min token length\n",
    "                    if len(tokens) < 5:\n",
    "                        logger.info(\"Text does not contain min. number of tokens, not adding\")\n",
    "                        self.count_filtered+=1\n",
    "                        continue\n",
    "\n",
    "                    # check ignored tokens from config\n",
    "                    for t in nltk_tokens_ignored:\n",
    "                        if t in tokens:\n",
    "                            logger.info(\"Text contains token from ignore list, not adding\")\n",
    "                            self.count_filtered+=1\n",
    "                            continue\n",
    "                    # check required tokens from config\n",
    "                    tokenspass = False\n",
    "                    for t in nltk_tokens_required:\n",
    "                        if t in tokens:\n",
    "                            tokenspass = True\n",
    "                            break\n",
    "                    if not tokenspass:\n",
    "                        logger.info(\"Text does not contain token from required list, not adding\")\n",
    "                        self.count_filtered+=1\n",
    "                        continue\n",
    "\n",
    "                    # get sentiment values\n",
    "                    polarity, subjectivity, sentiment = sentiment_analysis(htext)\n",
    "\n",
    "                    logger.info(\"Adding news headline to elasticsearch\")\n",
    "                    # add news headline data and sentiment info to elasticsearch\n",
    "                    es.index(index=args.index,\n",
    "                            doc_type=\"newsheadline\",\n",
    "                            body={\"date\": datenow,\n",
    "                                \"location\": htext_url,\n",
    "                                \"message\": htext,\n",
    "                                \"polarity\": polarity,\n",
    "                                \"subjectivity\": subjectivity,\n",
    "                                \"sentiment\": sentiment})\n",
    "\n",
    "            logger.info(\"Will get news headlines again in %s sec...\" % self.frequency)\n",
    "            time.sleep(self.frequency)\n",
    "\n",
    "    def get_news_headlines(self, url):\n",
    "\n",
    "        latestheadlines = []\n",
    "        latestheadlines_links = []\n",
    "        parsed_uri = urlparse.urljoin(url, '/')\n",
    "\n",
    "        try:\n",
    "\n",
    "            req = requests.get(url)\n",
    "            html = req.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            html = soup.findAll('h3')\n",
    "            links = soup.findAll('a')\n",
    "\n",
    "            logger.debug(html)\n",
    "            logger.debug(links)\n",
    "\n",
    "            if html:\n",
    "                for i in html:\n",
    "                    latestheadlines.append((i.next.next.next.next, url))\n",
    "            logger.debug(latestheadlines)\n",
    "\n",
    "            if args.followlinks:\n",
    "                if links:\n",
    "                    for i in links:\n",
    "                        if '/news/' in i['href']:\n",
    "                            l = parsed_uri.rstrip('/') + i['href']\n",
    "                            if l not in self.followedlinks:\n",
    "                                latestheadlines_links.append(l)\n",
    "                                self.followedlinks.append(l)\n",
    "                logger.debug(latestheadlines_links)\n",
    "\n",
    "                logger.info(\"Following any new links and grabbing text from page...\")\n",
    "\n",
    "                for linkurl in latestheadlines_links:\n",
    "                    for p in get_page_text(linkurl):\n",
    "                        latestheadlines.append((p, linkurl))\n",
    "                logger.debug(latestheadlines)\n",
    "\n",
    "        except requests.exceptions.RequestException as re:\n",
    "            logger.warning(\"Exception: can't crawl web site (%s)\" % re)\n",
    "            pass\n",
    "\n",
    "        return latestheadlines\n",
    "\n",
    "\n",
    "def get_page_text(url):\n",
    "\n",
    "    max_paragraphs = 10\n",
    "\n",
    "    try:\n",
    "        logger.debug(url)\n",
    "        req = requests.get(url)\n",
    "        html = req.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        html_p = soup.findAll('p')\n",
    "\n",
    "        logger.debug(html_p)\n",
    "\n",
    "        if html_p:\n",
    "            n = 1\n",
    "            for i in html_p:\n",
    "                if n <= max_paragraphs:\n",
    "                    if i.string is not None:\n",
    "                        logger.debug(i.string)\n",
    "                        yield i.string\n",
    "                n += 1\n",
    "\n",
    "    except requests.exceptions.RequestException as re:\n",
    "        logger.warning(\"Exception: can't crawl web site (%s)\" % re)\n",
    "        pass\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # clean up text\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"https?\\S+\", \"\", text)\n",
    "    text = re.sub(r\"&.*?;\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = text.replace(\"RT\", \"\")\n",
    "    text = text.replace(u\"â€¦\", \"\")\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text_sentiment(text):\n",
    "    # clean up text for sentiment analysis\n",
    "    text = re.sub(r\"[#|@]\\S+\", \"\", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_sentiment_from_url(text, sentimentURL):\n",
    "    # get sentiment from text processing website\n",
    "    payload = {'text': text}\n",
    "\n",
    "    try:\n",
    "        #logger.debug(text)\n",
    "        post = requests.post(sentimentURL, data=payload)\n",
    "        #logger.debug(post.status_code)\n",
    "        #logger.debug(post.text)\n",
    "    except requests.exceptions.RequestException as re:\n",
    "        logger.error(\"Exception: requests exception getting sentiment from url caused by %s\" % re)\n",
    "        raise\n",
    "\n",
    "    # return None if we are getting throttled or other connection problem\n",
    "    if post.status_code != 200:\n",
    "        logger.warning(\"Can't get sentiment from url caused by %s %s\" % (post.status_code, post.text))\n",
    "        return None\n",
    "\n",
    "    response = post.json()\n",
    "\n",
    "    neg = response['probability']['neg']\n",
    "    pos = response['probability']['pos']\n",
    "    neu = response['probability']['neutral']\n",
    "    label = response['label']\n",
    "\n",
    "    # determine if sentiment is positive, negative, or neutral\n",
    "    if label == \"neg\":\n",
    "        sentiment = \"negative\"\n",
    "    elif label == \"neutral\":\n",
    "        sentiment = \"neutral\"\n",
    "    else:\n",
    "        sentiment = \"positive\"\n",
    "\n",
    "    return sentiment, neg, pos, neu\n",
    "\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    \"\"\"Determine if sentiment is positive, negative, or neutral\n",
    "    algorithm to figure out if sentiment is positive, negative or neutral\n",
    "    uses sentiment polarity from TextBlob, VADER Sentiment and\n",
    "    sentiment from text-processing URL\n",
    "    could be made better :)\n",
    "    \"\"\"\n",
    "\n",
    "    # pass text into sentiment url\n",
    "    if args.websentiment:\n",
    "        ret = get_sentiment_from_url(text, sentimentURL)\n",
    "        if ret is None:\n",
    "            sentiment_url = None\n",
    "        else:\n",
    "            sentiment_url, neg_url, pos_url, neu_url = ret\n",
    "    else:\n",
    "        sentiment_url = None\n",
    "\n",
    "    # pass text into TextBlob\n",
    "    text_tb = TextBlob(text)\n",
    "\n",
    "    # pass text into VADER Sentiment\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    text_vs = analyzer.polarity_scores(text)\n",
    "\n",
    "    # determine sentiment from our sources\n",
    "    if sentiment_url is None:\n",
    "        if text_tb.sentiment.polarity < 0 and text_vs['compound'] <= -0.05:\n",
    "            sentiment = \"negative\"\n",
    "        elif text_tb.sentiment.polarity > 0 and text_vs['compound'] >= 0.05:\n",
    "            sentiment = \"positive\"\n",
    "        else:\n",
    "            sentiment = \"neutral\"\n",
    "    else:\n",
    "        if text_tb.sentiment.polarity < 0 and text_vs['compound'] <= -0.05 and sentiment_url == \"negative\":\n",
    "            sentiment = \"negative\"\n",
    "        elif text_tb.sentiment.polarity > 0 and text_vs['compound'] >= 0.05 and sentiment_url == \"positive\":\n",
    "            sentiment = \"positive\"\n",
    "        else:\n",
    "            sentiment = \"neutral\"\n",
    "\n",
    "    # calculate average polarity from TextBlob and VADER\n",
    "    polarity = (text_tb.sentiment.polarity + text_vs['compound']) / 2\n",
    "\n",
    "    # output sentiment polarity\n",
    "    print(\"************\")\n",
    "    print(\"Sentiment Polarity: \" + str(round(polarity, 3)))\n",
    "\n",
    "    # output sentiment subjectivity (TextBlob)\n",
    "    print(\"Sentiment Subjectivity: \" + str(round(text_tb.sentiment.subjectivity, 3)))\n",
    "\n",
    "    # output sentiment\n",
    "    print(\"Sentiment (url): \" + str(sentiment_url))\n",
    "    print(\"Sentiment (algorithm): \" + str(sentiment))\n",
    "    print(\"Overall sentiment (textblob): \", text_tb.sentiment) \n",
    "    print(\"Overall sentiment (vader): \", text_vs) \n",
    "    print(\"sentence was rated as \", round(text_vs['neg']*100, 3), \"% Negative\") \n",
    "    print(\"sentence was rated as \", round(text_vs['neu']*100, 3), \"% Neutral\") \n",
    "    print(\"sentence was rated as \", round(text_vs['pos']*100, 3), \"% Positive\") \n",
    "    print(\"************\")\n",
    "\n",
    "    return polarity, text_tb.sentiment.subjectivity, sentiment\n",
    "\n",
    "\n",
    "def tweeklink_sentiment_analysis(url):\n",
    "    # get text summary of tweek link web page and run sentiment analysis on it\n",
    "    try:\n",
    "        logger.info('Following tweet link %s to get sentiment..' % url)\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        # check if twitter web page\n",
    "        if \"Tweet with a location\" in article.text:\n",
    "            logger.info('Link to Twitter web page, skipping')\n",
    "            return None\n",
    "        article.nlp()\n",
    "        tokens = article.keywords\n",
    "        print(\"Tweet link nltk tokens:\", tokens)\n",
    "\n",
    "        # check for min token length\n",
    "        if len(tokens) < 5:\n",
    "            logger.info(\"Tweet link does not contain min. number of tokens, not adding\")\n",
    "            return None\n",
    "        # check ignored tokens from config\n",
    "        for t in nltk_tokens_ignored:\n",
    "            if t in tokens:\n",
    "                logger.info(\"Tweet link contains token from ignore list, not adding\")\n",
    "                return None\n",
    "        # check required tokens from config\n",
    "        tokenspass = False\n",
    "        tokensfound = 0\n",
    "        for t in nltk_tokens_required:\n",
    "            if t in tokens:\n",
    "                tokensfound += 1\n",
    "                if tokensfound == nltk_min_tokens:\n",
    "                    tokenspass = True\n",
    "                    break\n",
    "        if not tokenspass:\n",
    "            logger.info(\"Tweet link does not contain token from required list or min required, not adding\")\n",
    "            return None\n",
    "\n",
    "        summary = article.summary\n",
    "        if summary == '':\n",
    "            logger.info('No text found in tweet link url web page')\n",
    "            return None\n",
    "        summary_clean = clean_text(summary)\n",
    "        summary_clean = clean_text_sentiment(summary_clean)\n",
    "        print(\"Tweet link Clean Summary (sentiment): \" + summary_clean)\n",
    "        polarity, subjectivity, sentiment = sentiment_analysis(summary_clean)\n",
    "        \n",
    "        return polarity, subjectivity, sentiment\n",
    "\n",
    "    except ArticleException as e:\n",
    "        logger.warning('Exception: error getting text on Twitter link caused by: %s' % e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_twitter_users_from_url(url):\n",
    "    twitter_users = []\n",
    "    logger.info(\"Grabbing any twitter users from url %s\" % url)\n",
    "    try:\n",
    "        twitter_urls = (\"http://twitter.com/\", \"http://www.twitter.com/\",\n",
    "                        \"https://twitter.com/\", \"https://www.twitter.com/\")\n",
    "        # req_header = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Safari/604.1.38\"}\n",
    "        req = requests.get(url)\n",
    "        html = req.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        html_links = []\n",
    "        for link in soup.findAll('a'):\n",
    "            html_links.append(link.get('href'))\n",
    "        if html_links:\n",
    "            for link in html_links:\n",
    "                # check if twitter_url in link\n",
    "                parsed_uri = urlparse.urljoin(link, '/')\n",
    "                # get twitter user name from link and add to list\n",
    "                if parsed_uri in twitter_urls and \"=\" not in link and \"?\" not in link:\n",
    "                    user = link.split('/')[3]\n",
    "                    twitter_users.append(u'@' + user)\n",
    "            logger.debug(twitter_users)\n",
    "    except requests.exceptions.RequestException as re:\n",
    "        logger.warning(\"Requests exception: can't crawl web site caused by: %s\" % re)\n",
    "        pass\n",
    "    return twitter_users\n",
    "\n",
    "\n",
    "def get_twitter_users_from_file(file):\n",
    "    # get twitter user ids from text file\n",
    "    twitter_users = []\n",
    "    logger.info(\"Grabbing any twitter user ids from file %s\" % file)\n",
    "    try:\n",
    "        f = open(file, \"rt\", encoding='utf-8')\n",
    "        for line in f.readlines():\n",
    "            u = line.strip()\n",
    "            twitter_users.append(u)\n",
    "        logger.debug(twitter_users)\n",
    "        f.close()\n",
    "    except (IOError, OSError) as e:\n",
    "        logger.warning(\"Exception: error opening file caused by: %s\" % e)\n",
    "        pass\n",
    "    return twitter_users\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # parse cli args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-i\", \"--index\", metavar=\"INDEX\", default=\"stocksight\",\n",
    "                        help=\"Index name for Elasticsearch (default: stocksight)\")\n",
    "    parser.add_argument(\"-d\", \"--delindex\", action=\"store_true\",\n",
    "                        help=\"Delete existing Elasticsearch index first\")\n",
    "    parser.add_argument(\"-s\", \"--symbol\", metavar=\"SYMBOL\", required=True,\n",
    "                        help=\"Stock symbol you are interesed in searching for, example: TSLA\")\n",
    "    parser.add_argument(\"-k\", \"--keywords\", metavar=\"KEYWORDS\",\n",
    "                        help=\"Use keywords to search for in Tweets instead of feeds. \"\n",
    "                             \"Separated by comma, case insensitive, spaces are ANDs commas are ORs. \"\n",
    "                             \"Example: TSLA,'Elon Musk',Musk,Tesla,SpaceX\")\n",
    "    parser.add_argument(\"-a\", \"--addtokens\", action=\"store_true\",\n",
    "                        help=\"Add nltk tokens required from config to keywords\")\n",
    "    parser.add_argument(\"-u\", \"--url\", metavar=\"URL\",\n",
    "                        help=\"Use twitter users from any links in web page at url\")\n",
    "    parser.add_argument(\"-f\", \"--file\", metavar=\"FILE\",\n",
    "                        help=\"Use twitter user ids from file\")\n",
    "    parser.add_argument(\"-l\", \"--linksentiment\", action=\"store_true\",\n",
    "                        help=\"Follow any link url in tweets and analyze sentiment on web page\")\n",
    "    parser.add_argument(\"-n\", \"--newsheadlines\", action=\"store_true\",\n",
    "                        help=\"Get news headlines instead of Twitter using stock symbol from -s\")\n",
    "    parser.add_argument(\"--frequency\", metavar=\"FREQUENCY\", default=120, type=int,\n",
    "                        help=\"How often in seconds to retrieve news headlines (default: 120 sec)\")\n",
    "    parser.add_argument(\"--followlinks\", action=\"store_true\",\n",
    "                        help=\"Follow links on news headlines and scrape relevant text from landing page\") \n",
    "    parser.add_argument(\"-w\", \"--websentiment\", action=\"store_true\",\n",
    "                        help=\"Get sentiment results from text processing website\")                  \n",
    "    parser.add_argument(\"--overridetokensreq\", metavar=\"TOKEN\", nargs=\"+\",\n",
    "                        help=\"Override nltk required tokens from config, separate with space\")\n",
    "    parser.add_argument(\"--overridetokensignore\", metavar=\"TOKEN\", nargs=\"+\",\n",
    "                        help=\"Override nltk ignore tokens from config, separate with space\")\n",
    "    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\",\n",
    "                       help=\"Increase output verbosity\")\n",
    "    parser.add_argument(\"--debug\", action=\"store_true\",\n",
    "                        help=\"Debug message output\")\n",
    "    parser.add_argument(\"-q\", \"--quiet\", action=\"store_true\",\n",
    "                        help=\"Run quiet with no message output\")\n",
    "    parser.add_argument(\"-V\", \"--version\", action=\"version\",\n",
    "                        version=\"stocksight v%s\" % STOCKSIGHT_VERSION,\n",
    "                        help=\"Prints version and exits\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # set up logging\n",
    "    logger = logging.getLogger('stocksight')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    eslogger = logging.getLogger('elasticsearch')\n",
    "    eslogger.setLevel(logging.WARNING)\n",
    "    tweepylogger = logging.getLogger('tweepy')\n",
    "    tweepylogger.setLevel(logging.INFO)\n",
    "    requestslogger = logging.getLogger('requests')\n",
    "    requestslogger.setLevel(logging.INFO)\n",
    "    logging.addLevelName(\n",
    "        logging.INFO, \"\\033[1;32m%s\\033[1;0m\"\n",
    "                      % logging.getLevelName(logging.INFO))\n",
    "    logging.addLevelName(\n",
    "        logging.WARNING, \"\\033[1;31m%s\\033[1;0m\"\n",
    "                         % logging.getLevelName(logging.WARNING))\n",
    "    logging.addLevelName(\n",
    "        logging.ERROR, \"\\033[1;41m%s\\033[1;0m\"\n",
    "                       % logging.getLevelName(logging.ERROR))\n",
    "    logging.addLevelName(\n",
    "        logging.DEBUG, \"\\033[1;33m%s\\033[1;0m\"\n",
    "                       % logging.getLevelName(logging.DEBUG))\n",
    "    logformatter = '%(asctime)s [%(levelname)s][%(name)s] %(message)s'\n",
    "    loglevel = logging.INFO\n",
    "    logging.basicConfig(format=logformatter, level=loglevel)\n",
    "    if args.verbose:\n",
    "        logger.setLevel(logging.INFO)\n",
    "        eslogger.setLevel(logging.INFO)\n",
    "        tweepylogger.setLevel(logging.INFO)\n",
    "        requestslogger.setLevel(logging.INFO)\n",
    "    if args.debug:\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        eslogger.setLevel(logging.DEBUG)\n",
    "        tweepylogger.setLevel(logging.DEBUG)\n",
    "        requestslogger.setLevel(logging.DEBUG)\n",
    "    if args.quiet:\n",
    "        logger.disabled = True\n",
    "        eslogger.disabled = True\n",
    "        tweepylogger.disabled = True\n",
    "        requestslogger.disabled = True\n",
    "\n",
    "    # print banner\n",
    "    if not args.quiet:\n",
    "        c = randint(1, 4)\n",
    "        if c == 1:\n",
    "            color = '31m'\n",
    "        elif c == 2:\n",
    "            color = '32m'\n",
    "        elif c == 3:\n",
    "            color = '33m'\n",
    "        elif c == 4:\n",
    "            color = '35m'\n",
    "\n",
    "        banner = \"\"\"\\033[%s\n",
    "       _                     _                 \n",
    "     _| |_ _           _   _| |_ _     _   _   \n",
    "    |   __| |_ ___ ___| |_|   __|_|___| |_| |_ \n",
    "    |__   |  _| . |  _| '_|__   | | . |   |  _|\n",
    "    |_   _|_| |___|___|_,_|_   _|_|_  |_|_|_|  \n",
    "      |_|                   |_|   |___|                \n",
    "          :) = +$   :( = -$    v%s\n",
    "     https://github.com/shirosaidev/stocksight\n",
    "            \\033[0m\"\"\" % (color, STOCKSIGHT_VERSION)\n",
    "        print(banner + '\\n')\n",
    "\n",
    "    # create instance of elasticsearch\n",
    "    es = Elasticsearch(hosts=[{'host': elasticsearch_host, 'port': elasticsearch_port}],\n",
    "                http_auth=(elasticsearch_user, elasticsearch_password))\n",
    "\n",
    "    # set up elasticsearch mappings and create index\n",
    "    mappings = {\n",
    "        \"mappings\": {\n",
    "            \"tweet\": {\n",
    "                \"properties\": {\n",
    "                    \"author\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"fields\": {\n",
    "                            \"keyword\": {\n",
    "                                \"type\": \"keyword\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"fields\": {\n",
    "                            \"keyword\": {\n",
    "                                \"type\": \"keyword\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"language\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"fields\": {\n",
    "                            \"keyword\": {\n",
    "                                \"type\": \"keyword\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"friends\": {\n",
    "                        \"type\": \"long\"\n",
    "                    },\n",
    "                    \"followers\": {\n",
    "                        \"type\": \"long\"\n",
    "                    },\n",
    "                    \"statuses\": {\n",
    "                        \"type\": \"long\"\n",
    "                    },\n",
    "                    \"date\": {\n",
    "                        \"type\": \"date\"\n",
    "                    },\n",
    "                    \"message\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"fields\": {\n",
    "                            \"english\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"analyzer\": \"english\"\n",
    "                            },\n",
    "                            \"keyword\": {\n",
    "                                \"type\": \"keyword\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"tweet_id\": {\n",
    "                        \"type\": \"long\"\n",
    "                    },\n",
    "                    \"polarity\": {\n",
    "                        \"type\": \"float\"\n",
    "                    },\n",
    "                    \"subjectivity\": {\n",
    "                        \"type\": \"float\"\n",
    "                    },\n",
    "                    \"sentiment\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"fields\": {\n",
    "                            \"keyword\": {\n",
    "                                \"type\": \"keyword\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"newsheadline\": {\n",
    "                \"properties\": {\n",
    "                    \"date\": {\n",
    "                        \"type\": \"date\"\n",
    "                    },\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"fields\": {\n",
    "                            \"keyword\": {\n",
    "                                \"type\": \"keyword\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"message\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"fields\": {\n",
    "                            \"english\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"analyzer\": \"english\"\n",
    "                            },\n",
    "                            \"keyword\": {\n",
    "                                \"type\": \"keyword\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"polarity\": {\n",
    "                        \"type\": \"float\"\n",
    "                    },\n",
    "                    \"subjectivity\": {\n",
    "                        \"type\": \"float\"\n",
    "                    },\n",
    "                    \"sentiment\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"fields\": {\n",
    "                            \"keyword\": {\n",
    "                                \"type\": \"keyword\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if args.delindex:\n",
    "        logger.info('Deleting existing Elasticsearch index ' + args.index)\n",
    "        es.indices.delete(index=args.index, ignore=[400, 404])\n",
    "\n",
    "    logger.info('Creating new Elasticsearch index or using existing ' + args.index)\n",
    "    es.indices.create(index=args.index, body=mappings, ignore=[400, 404])\n",
    "\n",
    "    # check if we need to override any tokens\n",
    "    if args.overridetokensreq:\n",
    "        nltk_tokens_required = tuple(args.overridetokensreq)\n",
    "    if args.overridetokensignore:\n",
    "        nltk_tokens_ignored = tuple(args.overridetokensignore)\n",
    "\n",
    "    # are we grabbing news headlines from yahoo finance or twitter\n",
    "    if args.newsheadlines:\n",
    "        try:\n",
    "            url = \"https://finance.yahoo.com/quote/%s/?p=%s\" % (args.symbol, args.symbol)\n",
    "\n",
    "            logger.info('NLTK tokens required: ' + str(nltk_tokens_required))\n",
    "            logger.info('NLTK tokens ignored: ' + str(nltk_tokens_ignored))\n",
    "            logger.info(\"Scraping news for %s from %s ...\" % (args.symbol, url))\n",
    "\n",
    "            # create instance of NewsHeadlineListener\n",
    "            newslistener = NewsHeadlineListener(url, args.frequency)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Ctrl-c keyboard interrupt, exiting...\")\n",
    "            sys.exit(0)\n",
    "\n",
    "    else:\n",
    "        # create instance of the tweepy tweet stream listener\n",
    "        tweetlistener = TweetStreamListener()\n",
    "\n",
    "        # set twitter keys/tokens\n",
    "        auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "        api = API(auth)\n",
    "\n",
    "        # create instance of the tweepy stream\n",
    "        stream = Stream(auth, tweetlistener)\n",
    "\n",
    "        # grab any twitter users from links in web page at url\n",
    "        if args.url:\n",
    "            twitter_users = get_twitter_users_from_url(args.url)\n",
    "            if len(twitter_users) > 0:\n",
    "                twitter_feeds = twitter_users\n",
    "            else:\n",
    "                logger.info(\"No twitter users found in links on web page, exiting\")\n",
    "                sys.exit(1)\n",
    "\n",
    "        # grab twitter users from file\n",
    "        if args.file:\n",
    "            twitter_users = get_twitter_users_from_file(args.file)\n",
    "            if len(twitter_users) > 0:\n",
    "                useridlist = twitter_users\n",
    "            else:\n",
    "                logger.info(\"No twitter users found in file, exiting\")\n",
    "                sys.exit(1)\n",
    "        elif args.keywords is None:\n",
    "            # build user id list from user names\n",
    "            logger.info(\"Looking up Twitter user ids from usernames... (use -f twitteruserids.txt for cached user ids)\")\n",
    "            useridlist = []\n",
    "            while True:\n",
    "                for u in twitter_feeds:\n",
    "                    try:\n",
    "                        # get user id from screen name using twitter api\n",
    "                        user = api.get_user(screen_name=u)\n",
    "                        uid = str(user.id)\n",
    "                        if uid not in useridlist:\n",
    "                            useridlist.append(uid)\n",
    "                        time.sleep(randrange(2, 5))\n",
    "                    except TweepError as te:\n",
    "                        # sleep a bit in case twitter suspends us\n",
    "                        logger.warning(\"Tweepy exception: twitter api error caused by: %s\" % te)\n",
    "                        logger.info(\"Sleeping for a random amount of time and retrying...\")\n",
    "                        time.sleep(randrange(2,30))\n",
    "                        continue\n",
    "                    except KeyboardInterrupt:\n",
    "                        logger.info(\"Ctrl-c keyboard interrupt, exiting...\")\n",
    "                        stream.disconnect()\n",
    "                        sys.exit(0)\n",
    "                break\n",
    "\n",
    "            if len(useridlist) > 0:\n",
    "                logger.info('Writing twitter user ids to text file %s' % twitter_users_file)\n",
    "                try:\n",
    "                    f = open(twitter_users_file, \"wt\", encoding='utf-8')\n",
    "                    for i in useridlist:\n",
    "                        line = str(i) + \"\\n\"\n",
    "                        if type(line) is bytes:\n",
    "                            line = line.decode('utf-8')\n",
    "                        f.write(line)\n",
    "                    f.close()\n",
    "                except (IOError, OSError) as e:\n",
    "                    logger.warning(\"Exception: error writing to file caused by: %s\" % e)\n",
    "                    pass\n",
    "                except Exception as e:\n",
    "                    raise\n",
    "\n",
    "        try:\n",
    "            # search twitter for keywords\n",
    "            logger.info('Stock symbol: ' + str(args.symbol))\n",
    "            logger.info('NLTK tokens required: ' + str(nltk_tokens_required))\n",
    "            logger.info('NLTK tokens ignored: ' + str(nltk_tokens_ignored))\n",
    "            logger.info('Listening for Tweets (ctrl-c to exit)...')\n",
    "            if args.keywords is None:\n",
    "                logger.info('No keywords entered, following Twitter users...')\n",
    "                logger.info('Twitter Feeds: ' + str(twitter_feeds))\n",
    "                logger.info('Twitter User Ids: ' + str(useridlist))\n",
    "                stream.filter(follow=useridlist, languages=['en'])\n",
    "            else:\n",
    "                # keywords to search on twitter\n",
    "                # add keywords to list\n",
    "                keywords = args.keywords.split(',')\n",
    "                if args.addtokens:\n",
    "                    # add tokens to keywords to list\n",
    "                    for f in nltk_tokens_required:\n",
    "                        keywords.append(f)\n",
    "                logger.info('Searching Twitter for keywords...')\n",
    "                logger.info('Twitter keywords: ' + str(keywords))\n",
    "                stream.filter(track=keywords, languages=['en'])\n",
    "        except TweepError as te:\n",
    "            logger.debug(\"Tweepy Exception: Failed to get tweets caused by: %s\" % te)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Ctrl-c keyboard interrupt, exiting...\")\n",
    "            stream.disconnect()\n",
    "            sys.exit(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
